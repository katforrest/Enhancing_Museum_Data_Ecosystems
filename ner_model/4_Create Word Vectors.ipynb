{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b245bf51",
   "metadata": {},
   "source": [
    "As part of training a model, I included word vectors in the training data to improve performace. \n",
    "The word vectors were created based on text from the original research publications and saved as word_vecs.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e321a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "from random import sample\n",
    "import json\n",
    "import jsonlines\n",
    "import pysbd\n",
    "seg = pysbd.Segmenter(language='en', clean=False)\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ed925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vectors using 400 TEST papers\n"
     ]
    }
   ],
   "source": [
    "papers_train = glob.glob('./papers_train/*')\n",
    "print(f\"Creating Vectors using {len(papers_train)} TEST papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021dfae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull data from the publications' reference sections\n",
    "\n",
    "def RefEntriesParse(file):\n",
    "    with open(file, 'r') as f:\n",
    "        reflist = []    \n",
    "        data = json.loads(f.read()) ## dictionary\n",
    "        for k,v in data[\"pdf_parse\"][\"ref_entries\"].items():\n",
    "            if k.startswith(\"FIGREF\") or k.startswith(\"TABREF\"):\n",
    "                ref_label = k\n",
    "                reflist.append(ref_label)\n",
    "    return(reflist)\n",
    "\n",
    "def json2text(file, refID):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "        ref_text_df = pd.json_normalize(data[\"pdf_parse\"][\"ref_entries\"][refID])\n",
    "    return(ref_text_df) \n",
    "\n",
    "def text2sentences(ref_text_df):\n",
    "    sentences_text = \" \".join(list(ref_text_df.text))  #sentences_text is a string\n",
    "    return(sentences_text)\n",
    "\n",
    "def sentences2sentencelist(text):\n",
    "    sentences = seg.segment(text) #sentences is a list of strings\n",
    "    sentences = [re.sub(r\"^\\W+\", \"\", sentence) for sentence in sentences] \n",
    "    sentences = [re.sub(r\"\\s+\", \" \", sentence) for sentence in sentences]\n",
    "    return(sentences) #sentences is a list of strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b195b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist =  []\n",
    "for file in papers_train:\n",
    "    reflist = RefEntriesParse(file)\n",
    "    for refID in reflist:\n",
    "        try:\n",
    "            ref_text_df = json2text(file, refID)\n",
    "            sentences_text = text2sentences(ref_text_df)\n",
    "            sentencelist = sentences2sentencelist(sentences_text)\n",
    "            mylist.extend(sentencelist)\n",
    "        except:\n",
    "            print(file + \" returns error.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d87952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21795\n"
     ]
    }
   ],
   "source": [
    "print(len(mylist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7918efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull data from the publications' body text\n",
    "\n",
    "def json2text(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "        body_text_df = pd.json_normalize(data[\"pdf_parse\"][\"body_text\"])   #body_text_df is a dataframe object \n",
    "    return(body_text_df) #body_text_df is a dataframe object \n",
    "\n",
    "def text2sentences(body_text_df):\n",
    "    sentences_text = \" \".join(list(body_text_df.text))  #sentences_text is a string\n",
    "    #pattern_brackets = re.compile(r'\\(.*?\\)')\n",
    "    #sentences_text = re.sub(pattern_brackets, \"\", sentences_text)\n",
    "    return(sentences_text) #sentences_text still a string\n",
    "\n",
    "def sentences2sentencelist(text):\n",
    "    sentences = seg.segment(text) #sentences is a list of strings\n",
    "    sentences = [re.sub(r\"^\\W+\", \"\", sentence) for sentence in sentences] \n",
    "    sentences = [re.sub(r\"\\s+\", \" \", sentence) for sentence in sentences]\n",
    "    return(sentences) #sentences is a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73346e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./papers_train\\0e8472a7-dd33-2379-a88f-58c1237d7b90.json returns error.\n"
     ]
    }
   ],
   "source": [
    "for file in papers_train:\n",
    "    try:\n",
    "        body_text_df = json2text(file) #body_text_df is a dataframe object\n",
    "        sentencetext = text2sentences(body_text_df) #sentencetext is a string\n",
    "        sentencelist = sentences2sentencelist(sentencetext) #Sentencelist is a list of strings\n",
    "        mylist.extend(sentencelist)\n",
    "    except:\n",
    "        print(file+ \" returns error.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c4f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115544\n"
     ]
    }
   ],
   "source": [
    "print(len(mylist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "795fb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start creating word vectors\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac19a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exclude common stopwords\n",
    "\n",
    "stopwords = [\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\n",
    "             \"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\n",
    "             \"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\n",
    "             \"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\n",
    "             \"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\n",
    "             \"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\n",
    "             \"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\n",
    "             \"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\n",
    "             \"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d810d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac07390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82153427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chipmunk of the quadrivittatus group (see A. H. Howell, Revision of the American Chipmunks, North American Fauna, no. 52, pp.\n",
      "<class 'list'>\n",
      "115544\n"
     ]
    }
   ],
   "source": [
    "corpus_data = mylist\n",
    "        \n",
    "print(corpus_data[114])\n",
    "print(type(corpus_data))\n",
    "print(len(corpus_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c2b419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in corpus_data:\n",
    "    corpus_sentence = i\n",
    "    \n",
    "    corpus_sentence = corpus_sentence.lower()\n",
    "    words = corpus_sentence.split()\n",
    "    \n",
    "    new_corpus = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_corpus.append(word)\n",
    "    corpus_sentence = \" \".join(new_corpus)\n",
    "    ##print(corpus_sentence) ##strings\n",
    "    \n",
    "    doc = nlp(corpus_sentence)\n",
    "    for sent in doc.sents:\n",
    "        sentence = sent.text.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = sentence.split()\n",
    "        sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ad9c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126557\n",
      "['b', 'model', 'bform', 'dna', 'methylated', 'cytosines', 'two', 'selfcomplementary', 'cpg', 'sequences']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences)) \n",
    "print(sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d88af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126557\n",
      "Made Phrases\n",
      "Made Bigrams\n",
      "Found sentences\n",
      "103894\n",
      "Training model now...\n"
     ]
    }
   ],
   "source": [
    "def create_wordvecs(corpus, model_name):\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    from gensim.models.phrases import Phrases, Phraser\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    print (len(corpus))\n",
    "    \n",
    "\n",
    "    phrases = Phrases(corpus, min_count=30, progress_per=10000)\n",
    "    print (\"Made Phrases\")\n",
    "    \n",
    "    bigram = Phraser(phrases)\n",
    "    print (\"Made Bigrams\")\n",
    "    \n",
    "    sentences = phrases[corpus]  #uses phrases function on corpus\n",
    "    print (\"Found sentences\")\n",
    "    word_freq = defaultdict(int)\n",
    "\n",
    "    for sent in sentences:\n",
    "        for i in sent:\n",
    "            word_freq[i]+=1\n",
    "\n",
    "    print (len(word_freq))\n",
    "    print (\"Training model now...\")\n",
    "    w2v_model = Word2Vec(min_count=1,\n",
    "                        window=2,\n",
    "                        vector_size=100,\n",
    "                        sample=6e-5,\n",
    "                        alpha=0.03,\n",
    "                        min_alpha=0.0007,\n",
    "                        negative=20)\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "    w2v_model.wv.save_word2vec_format(f\"{model_name}.txt\")\n",
    "create_wordvecs(sentences, \"word_vecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85763328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103894 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open (\"word_vecs.txt\", \"r\", encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "    print (data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f2b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eg -0.563712 0.34385306 1.0083059 -1.0323424 0.28970072 0.40693393 -1.2952132 1.5048887 0.33374208 -0.32921082 0.73212796 -0.51552105 0.39277503 0.6580963 0.9097592 -0.3348809 -0.19832456 -0.62542444 -0.81215334 1.221862 0.7266369 -0.14894828 0.3656226 0.3982717 0.17695053 -0.105870664 -0.4493262 0.90788645 -0.6102477 0.46399173 0.003759885 -0.91160566 -0.09504749 -0.25322843 0.51737046 0.009254119 1.0340173 -0.7096859 0.6994959 0.2039257 -0.07492078 -0.8316059 1.0323558 0.35563767 0.43284747 0.14053516 0.51764256 -0.531015 0.76414245 -0.31208822 -0.4826614 0.44210315 -0.23457038 -0.29152867 0.37288314 0.15079196 -0.6035176 0.2249789 -0.06084129 0.5014859 -0.02693485 0.6821231 -0.45123282 -0.8941378 1.0836748 -0.001101461 0.253695 0.54476374 -0.083126426 -0.14241752 -0.48215535 -0.5296194 0.4788979 0.041297078 1.0733358 -0.7410006 0.6069829 0.51520765 -0.092621356 -0.4145046 -0.11635243 -0.24391577 0.3782005 -0.1578647 1.1697338 -0.69408745 -0.14193478 -0.17343818 -1.0569074 0.13193585 0.089809604 -1.0753616 0.031319946 -0.07929706 0.7544599 0.62942994 0.12671128 -0.59293866 -0.36698455 -0.63678604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (data[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9988a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
